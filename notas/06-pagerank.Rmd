# Pagerank y análisis de redes

## Introducción

**Pagerank** asigna un número real a cada página de una red (web). Este número es un indicador de su importancia. Las ideas fundamentales son: 

- Las páginas de internet forman una red o gráfica, donde los nodos son las páginas y las aristas dirigidas son las ligas de unas páginas a otras.
- La importancia de un página A depende de cuántas otras páginas apuntan a la página A. También depende de qué tan importantes sean las páginas que apuntan a A.
- Cuando hacemos una búsqueda, primero se filtran las páginas que tienen el contenido
de nuestra búsqueda, y después los resultados se ordenan según el pagerank
de estas páginas filtradas.
- ¿Qué problema resuelve? En un principio, se usaron métodos como índices y recuperación de documentos usando técnicas como tf-idf. El problema es que es muy fácil que un *spammer* sesgue los resultados para que sus páginas tengan alto nivel de relevancia en este sentido. Así que la importancia no se juzga sólo con el contenido,  sino de los *votos* de otras páginas importantes. Este es un sistema más difícil de engañar.

- Es crucial usar la importancia de los *in-links* de una página: si no, sería tambíen fácil crear muchas páginas spam que apunten a otra dada para aumentar
su importancia.

El *Pagerank*, más en general, es una medida de *centralidad* o *importancia* de los nodos de una red dirigida. Comenzaremos considerando redes más variadas (por ejemplo, redes sociales) y el concepto general de *centralidad*.

### Centralidad en redes 

Consideremos una red de personas, que representamos como una gráfica $G$ no dirigida. Las personas son los nodos y sus relaciones se representan con aristas no dirigidas.

Quiséramos construir una medida de importancia o centralidad de una persona dentro de la red. Por ejemplo:

- Redes sociales de internet: las ligas representan relación de *amigos*,
o la de *seguidor*.  Importancia: número de amigos o seguidores (grado de entrada o salida).
- Redes de citas bibliográficas: las ligas representan quién comparte o usa la información de quién. Importancia: número de citas o usos, ser citado por alguien importante, etc. 
- Red de empleados de una oficina: las ligas representan interacciones en algún periodo. Importancia: quién puede conectar de manera más inmediata a dos personas.


### Ejemplo de Moviegalaxies.com: Pulp Fiction {-}

 Dos personajes están ligados si tienen interacciones en la película. El tamaño y color de los nodos dependen de su "centralidad" en la red.

![Pulp fiction](./imagenes/pulpfiction.png)

(Gráfica creada con *Gephi*).

## Tipos de redes y su representación

Una red es un conjunto de *nodos* conectados por algunas *aristas*. 
Las aristas pueden ser 

- Dirigidas: hay un nodo origen y un nodo destino.
- No dirigidas: una arista representa una conexión simétrica entre dos nodos.

Podemos representar redes de varias maneras. Una primera manera
es con una lista de pares de *vértices* o *nodos* 
que están conectados por una *arista*. Por ejemplo, para una red dirigida:

```{r, fig.width=4, fig.asp=0.7, message = FALSE, warning = FALSE}
library(tidyverse)
library(tidygraph)
library(ggraph)
aristas <- data_frame(from = c(1, 1, 1, 1, 2), 
                      to =   c(2, 3, 4, 5, 3))
aristas
```

```{r}
red_tbl <- tidygraph::as_tbl_graph(aristas, directed = TRUE)
red_tbl
```

Que podemos visualizar como sigue:

```{r, fig.width = 6, fig.asp = 0.7}
graficar_red_dirigida <- function(red_tbl){
  ggraph(red_tbl) + 
    geom_edge_link(arrow = arrow(), end_cap = circle(4, 'mm')) +
    geom_node_point(size = 10, colour = 'salmon') +
    geom_node_text(aes(label = name)) +
    theme_graph() + coord_fixed()
}
graficar_red_dirigida(red_tbl)
```

También es posible representar una red mediante una **matriz de adyacencia**. 

La matriz de adyacencia para una red es la matriz $A$ tal que
$$A_{ij} = 1$$
si existe una arista de $i$ a $j$. En el caso dirigido, $A$ es una
matriz simétrica.

```{r}
matriz_ad <- igraph::get.adjacency(red_tbl)
matriz_ad
```

Es más conveniente representar estas matrices como matrices ralas,
como veremos más adelantes.

**Nota de R**: utilizamos el paquete *tidygraph* y **ggraph** para hacer manipulaciones de gráficas graficación. Estos paquetes son extensiones
del paquete  *igraph*, que es el que contiene los algoritmos de visualización, procesamiento
y resumen de redes.


## Visualización de redes

Existen varios algoritmos para visualizar redes que revelan distintos aspectos de su estructura (ver por ejemplo *?layout* en R, en el paquete igraph). 
Para examinar redes de tamaño relativamente
chico, el algoritmo de visualización es importante para determinar qué podemos
entender de la red.

Por ejemplo, aquí construimos una red aleatoria, y hacemos un *layout* de
nodos aleatorio:

```{r, fig.width = 6, fig.asp = 0.7}
set.seed(1234)
g <- play_erdos_renyi(n = 20, p = 0.1, directed = FALSE) %>% 
  as_tbl_graph()
ggraph(g, layout = 'randomly') +
  geom_edge_link() +
  geom_node_point(size = 2, colour = 'salmon') +
  theme_graph()
```

Y comparamos con un algoritmo basado en fuerzas.

```{r, fig.width = 6, fig.asp = 0.7}
ggraph(g, layout = 'kk') +
  geom_edge_link() +
  geom_node_point(size = 2, colour = 'salmon') +
  theme_graph()
```

```{block2, type='resumen'}
Los algoritmos basados en fuerzas para representar redes en 2 o 3 dimensiones se basan principalmente en la siguiente idea:

- Las aristas actúan como resortes, que no permiten que nodos ligados se alejen mucho
- Los nodos tienen fuerzas de repulsión entre ellos (la analogía física es de cargas elécricas), y también a veces de gravedad entre ellos.
- El algoritmo de representación intenta minimizar la energía de la configuración del sistema de atracciones y repulsiones.
```

Hay muchas variaciones de estos algoritmos, por ejemplo: *graphopt* en igraph, 
fruchtermann-rheingold, kamada-kawai, gem, escalamiento multidimensional, forceAtlas,
etc. Intenta mover los nodos de las siguiente gráfica para entender el funcionamiento
básico de estos algoritmos:



```{r}
library(visNetwork)
edges <- g %>% activate(edges) %>% as_data_frame
set.seed(13)
visNetwork(nodes = data_frame(id = 1:20, label = 1:20), 
           edges, 
           width = "100%") %>%
  visPhysics(solver ='forceAtlas2Based')
```


### Ejercicio
- Para la gráfica anterior, intenta usar los algoritmos *kk* o *fr*. Busca qué parámetros
puedes cambiar en el algoritmo y experimenta cambiándolos (cuánta repulsión, rigidez
de los resortes, número de iteraciones, etc.)


Otras familias de algoritmos intentan distintas estrategias, como los layout
de círculo, estrella, para árboles, etc.

```{r, fig.width = 6, fig.asp = 0.7}
ggraph(g, layout = 'circle') +
  geom_edge_link() +
  geom_node_point(size = 2, colour = 'salmon') +
  theme_graph()
```

## Medidas de centralidad para redes

Como discutimos arriba, las medidas de centralidad en redes intenta capturar
un concepto de importancia o conectividad de un nodo en una red. Primero comenzamos
con el caso **no dirigido**. Medidas básicas de centralidad son


- **Grado** o grado de entrada/salida: cuántas ligas tiene un nodo (no dirigidos, de entrada o de salida). 

- **Betweeness**: qué tan importante o única es un nodo para conectar otras dosnodos de la red (por ejemplo, una persona con betweeness alto controla más fácilmente el flujo de información en una red social). 
- **Cercanía**: qué tan lejos en promedio están los otros nodos de la red (pues puede encontrar y conectar más fácilmente otras dos en la red).

- **Centralidad de eigenvector/Pagerank**: la centralidad de un nodo es una especie de promedio de la centralidad de sus vecinos.

### Grado

Sea $G$ una gráfica **no dirigida**, y sea $A$ la matriz de adyacencia de $G$
Si $i$ es un nodo (vértice) dado, entonces su grado es

$$c_G(i)=\sum_{j\leq i} A_{i,j}.$$
que cuenta cúantas aristas conectan con el nodo $i$.

```{r}
graficar_red_nd <- function(dat_g){
  ggraph(dat_g, layout = 'kk') +
  geom_edge_link(alpha=0.2) +
  geom_node_point(aes(size = importancia), colour = 'salmon') +
  geom_node_text(aes(label = nombre), nudge_y = 0.2, size=3) +
  theme_graph(base_family = 'sans')
}

g_grado <- g %>% activate(nodes) %>%
  mutate(importancia = centrality_degree()) %>%
  mutate(nombre = 1:20) 

graficar_red_nd(g_grado)
```


#### ¿Qué no captura el grado como medida de centralidad? {-}

```{block2, type='resumen'}
El **grado** es una medida local que no toma en cuenta la topología más global
de la red: cómo están conectados nodos más lejanos alrededor del nodo que nos interesa.
```

#### Distancia a otros nodos {-}

En primer lugar, por ejemplo, no captura que algunos nodos están más cercanos en 
promedio a los nodos de la red que otros.

```{r}
g_simple <- igraph::graph(c(1, 2, 2, 3, 3, 4, 4, 5), directed = FALSE) %>% 
  as_tbl_graph() %>%
  mutate(importancia = centrality_degree()) %>%
  mutate(nombre  = LETTERS[1:5])
graficar_red_nd(g_simple)
```

Obsérvese en este ejemplo que el nodo $C$ es más importante que $D$, en el sentido
de que está más cercano a los nodos de toda la red, aún cuando el grado es el mismo
para ambos.


#### Caminos que pasan por un nodo {-}

En la siguiente gráfica, el nodo $G$ es importante porque es la única conexión
entre dos partes de la red, y esto no lo captura el grado:

```{r}
triangulo_1 <- c(1,2,2,3,3,1)
triangulo_2 <- triangulo_1 + 3
red_3 <- igraph::graph(c(triangulo_1, triangulo_2, c(7,1,7,4)), directed = FALSE) %>% 
  as_tbl_graph() %>%
  mutate(importancia = centrality_degree()) %>%
  mutate(nombre  = LETTERS[1:7])
graficar_red_nd(red_3)
```

#### Nodos conectados a otros nodos importantes {-}

En la siguiente gráfica el nodo $H$ tienen el mismo grado que $F$, pero
$H$ está conectado a un nodo más importante ($A$)
```{r}
red_4 <- igraph::graph(c(2,1,3,1,4,1,5,1,2,3,6,2,1,7,1,8), directed=FALSE) %>% 
  as_tbl_graph() %>%
  mutate(importancia = centrality_degree()) %>%
  mutate(nombre  = LETTERS[1:8])
graficar_red_nd(red_4)
```


### Medida de centralidad: *Betweeness*.

La medida de centralidad *betweeness* de un nodo $i$ se define como:
$$c_b (u) = \sum_{j<k, u\neq j,u\neq i} g(j,k |u) / g(j,k),$$
donde

- $g(j,k)$ es el número de caminos más cortos distintos entre $j$ y $k$ y 
- $g(j,k |i)$ es el número de caminos más cortos distintos entre $j$ y $k$ que pasan por $i$. 
- $g(j,k | i ) = 0$ cuando $j=i$ o $k=i$.

- Los caminos que más aportan al betweeness de un nodo $i$ son los que no tienen otra alternativa más que pasar por $i$.

Esta medida se puede normalizar poniendo ($n$ es el total de nodos de la red)
$$\overline{c}_b (i)=c_b (i)/\binom{n-1}{2},$$
pues el denominador es el máximo valor de betweeness que puede alcanzar un vértice
en una red de $n$ nodos. 

#### Ejemplo {-}

```{r, fig.width=4, align='center'}
red_4 <- igraph::graph(c(2,1,3,1,4,1,5,1,2,3,6,2,2,5,1,6,6,7), directed = FALSE) %>% 
  as_tbl_graph() %>%
  mutate(importancia = centrality_betweenness()) %>%
  mutate(nombre  = LETTERS[1:7])
graficar_red_nd(red_4) + labs(subtitle = 'Betweeness')
```

Por ejemplo, consideremos el nodo $B$. Hay dos caminos más
cortos de $C$ a $F$ (de tamaño) 2, y uno de ellos pasa por $B$. 
De modo que los caminos de $C$ a $F$ aportan 0.5 al *betweeness*
de $B$. De los caminos más cortos entre $E$ y $D$, ninguno pasa
por $B$, así que este par de vértices aporta 0 al *betweeness*.
Verifica el valor de betweeness para $B$ haciendo los cálculos
restantes:


```{r}
red_4 %>% as_data_frame
```

#### Ejemplo de grado y betweeness: Pulp Fiction {-}

En esta red, el color es una medición de betweeness y el tamaño el grado.
Aunque Butch y Jules tienen grados similares, Butch tiene *betweeness* más alto
pues provee más ligas únicas más cortas 
entre los personajes (mientras que la mayoría de los de Jules
pasan también por Vincent).

![Pulp fiction](./imagenes/pulp_fiction_between.png)


### Medida de centralidad: Cercanía

También es posible definir medidas de importancia según el promedio de cercanía a todos
los nodos: el inverso del promedio de distancias del nodo a todos los demás.

#### Ejemplo {-}
```{r}
red_5 <- igraph::graph(c(2,1,3,1,4,9,5,2,2,3,6,1,7,8,
                 8,9,9,1,1,8,1,7), 
               directed = FALSE)
```

```{r}
red_5 <- red_5 %>% as_tbl_graph() %>%
  mutate(importancia = centrality_closeness(normalized = TRUE)) %>%
  mutate(nombre  = LETTERS[1:9])
red_5 %>% activate(nodes) %>% as_data_frame
graficar_red_nd(red_5) + labs(subtitle = 'Cercanía')
```
En este ejemplo, el nodo $F$ tiene cercanía más alta que $D$, por ejemplo,
pues se conecta a un nodo bien conectado de la red (en grado y betweeness):

#### Ejercicio {-}
Verifica que la cercanía de $A$ es 0.80.

### Centralidad de eigenvector

Esta medida considera que la importancia de un nodo está dado por una suma de las
importancias de sus vecinos. De esta forma, es importante estar cercano a nodos importantes (como en cercanía), pero también cuenta conectarse a muchos nodos (como en grado).

- Nótese que esta es una descripción circular: para saber la importancia de un nodo, hay que saber la importancia de sus vecinos.

Consideremos el ejemplo siguiente:


```{r, fig.height=3}
red_6 <- igraph::graph(c(1,2,1,3,1,4,5,2), directed = FALSE) %>%
  as_tbl_graph() %>% mutate(nombre = 1:5, importancia = 0)
graficar_red_nd(red_6) + theme(legend.position="none")
```

Supongamos que las importancias  de estos 5 nodos son
$$(x_1,\ldots, x_5)$$
donde $x_i\geq 0$.  Suponemos además que consideramos las importancias
normalizadas, es decir,
$$\sum_i x_i = 1.$$

Por ejemplo, en una red con un solo nodo, este nodo tiene importancia 1, una con dos
nodos conectados  debe tener $(1/2,1/2)$ como importancias.

De acuerdo a la idea mencionada arriba, calculamos entonces cómo se ve la suma de las importancias de nodos adyacentes a cada nodo. Para el nodo uno,

$$\frac{y_1}{\lambda} = x_2 + x_3 + x_4$$

donde $\lambda = \sum_i  y_i$ es la constante de normalización. También podemos
escribir como

$${y_1} = \lambda(x_2 + x_3 + x_4)$$


para el nodo 2 (despejando $\lambda$)

$$y_2 = \lambda(x_1 + x_5)$$

y para los siguientes nodos tendríamos
$$y_3 = \lambda x_1$$
$$y_4 = \lambda x_1$$
$$y_5 = \lambda x_2$$.

Este sistema lo podemos escribir de forma matricial, usando
la matriz de adyacencia, como

\[ y = 
\lambda \left (
\begin{array}{rrrrr}
 0 & 1 & 1 & 1 & 0 \\ 
 1 & 0 & 0 & 0 & 1 \\ 
  1 & 0 & 0 & 0 & 0 \\ 
  1 & 0 & 0 & 0 & 0 \\ 
  0 & 1 & 0 & 0 & 0 \\ 
\end{array}
\right ) x
\]


**Por definición de las importancias**, si normalizamos este vector debemos obtener las importancias originales:

$$x = (x_1,\ldots, x_5)={\frac{1}{\lambda}}(y_1,\ldots,y_5)=\frac{1}{\lambda}y$$

En resumen, $x\geq 0$ debe satisfacer, para alguna $\lambda > 0$, la
ecuación ($A$ es simétrica):
$$A^t x = \lambda x,$$

es decir, $x$ es un vector propio de la matriz de adyacencia con valor propio positivo.

Sin embargo, ¿cuando existe un vector $x\geq 0$ con $\lambda>0$ que satisfaga esta propiedad?

#### Ejercicio {-}
Resuelve el sistema de ecuaciones de arriba y verifica que tal vector existe.
¿Cuál es el valor de lambda?



#### Matrices no negativas {-}

Para entender la existencia y forma de la centralidad de eigenvector,
comenzamos recordando algunos teoremas básicos de álgebra lineal. En primer
lugar, tenemos:

```{block2, type='resumen'}
**Espectro de matrices no-negativas**

Si $A$ es una matriz no negativa,  entonces:

- Existe un valor
propio real *no-negativo* $\lambda_0$ tal que $\lambda_0\geq |\lambda|$ para cualquier otro valor propio $\lambda$ de $A$. 
- Al valor propio $\lambda_0$ está asociado a un vector propio $x$ con entradas no negativas. 
```

Nota: Este teorema se puede entender observando que si $A$ es no-negativa, entonces
mapea el cono $\{(x_1,x_2,\ldots, x_m) | x_i \geq 0\}$ dentro de sí mismo,
lo que implica que debe dejar invariante alguna dirección dentro de este cono.

Si este vector propio no-negativo fuera único (hasta normalización) y distinto del
vector 0, entonces esto nos daría un conjunto de medidas (únicas hasta normalización) $x$ para la importancia de los nodos:

### Ejemplo 1

```{r fig.width=4}
par(mar=c(0,0,0,0)); plot(red_6, vertex.size=40)
A_red <- igraph::get.adjacency(red_6)
A_red
```


```{r}
desc_A <- eigen(A_red)
print(desc_A, digits = 2)
```

Las medida de centralidad de eigenvector da entonces, en este caso:

```{r, fig.width=4}
x <- desc_A$vectors[,1]
print(x, digits = 2)
par(mar=c(0,0,0,0)); plot(red_6, vertex.size  = 100*x)
```


#### Ejemplo 2 {-}

Sin embargo, puede ser que obtengamos más de un valor propio no negativo con
vectores asociados no negativos, por ejemplo:


```{r, fig.width=6, message=FALSE}
red <- igraph::graph(c(1,2,2,3,3,1,2,4,5,6), directed = FALSE)
par(mar=c(0,0,0,0)); plot(red, vertex.size=20)
A_red <- igraph::get.adjacency(red)
A_red
```


Nótese que los eigenvectores 1 y 2 son no negativos, y están asociados a 
vectores propios no negativos:
```{r}
desc_A <- eigen(A_red)
print(desc_A, digits = 2)
```

En este caso, la medida de centralización dependería de qué
peso le ponemos al primer vector propio vs el segundo vector propio. En este
ejemplo, la unicidad no sucede pues la red asociado no es conexa.

### Matrices irreducibles y gráficas fuertemente conexas {-}

¿Cuándo podemos garantizar unicidad en la solución de $Ax=\lambda x$ con $\lambda >0?$, y $x$ un vector no-negativo distinto de 0?

Sea $A$ la matriz de adyacencia de una gráfica no dirigida.
  
-  Si la gráfica asociada a $A$ es fuertemente conexa (existen caminos entre cualquier par de vértices) entonces decimos que $A$ es **irreducible**.
- Podemos dar también una definición de irreducibilidad sólo en términos
de $A$: $A$ es irreducible cuando para toda $i,j$ existe $m\geq 0$ tal
que $(A^m)_{i,j} > 0$.

*Nota*: discute por qué estas dos definiciones son equivaentes.

```{block2, type='resumen'}
**Teorema de Perron-Frobenius**
  
Si $A$ es una matriz no-negativa irreducible, entonces

- Existe un valor
propio real *positivo* $\lambda_0$ tal que $\lambda_0 > |\lambda|$ para cualquier otro valor propio $\lambda$ de $A$, asociado a un vector propio $x$ con entradas positivas.
- No existe ningún otro vector propio con entradas no negativas que no sea paralelo a $x$.
```

Y entonces podemos definir una medida única de centralidad módulo una constante multiplicativa.

```{block2, type="resumen"}
Si $A$ es la matriz de adyacencia de una red no dirigida, y $A$ es irreducible (significa que la red es fuertemente conexa), definimos
la **centralidad de eigenvector** de un nodo $i$ como la $i$-esima componente
del vector positivo $x$ (con $\sum x_i = 1$)
asociado al valor propio (único) de Perron-Frobenius. 
```

#### Ejemplo: facultad de tres universidades {-}

```{r}
install.packages('igraphdata')
library("igraphdata")
data("UKfaculty")
ukf.und <- igraph::as.undirected(UKfaculty) 
head(dat.1 <- igraph::get.data.frame((ukf.und)))
dat.1$from <- dat.1$from 
dat.1$to <- dat.1$to
grupo <- igraph::get.vertex.attribute(UKfaculty, 'Group')
nodos <- data.frame(id=1:length(grupo))

visNetwork(nodos, dat.1, width = "100%") %>%
  visPhysics(solver ='forceAtlas2Based', 
             forceAtlas2Based = list(gravitationalConstant = -10),
             stabilization = TRUE)
```



Ahora calculamos centralidad de eigenvector.

```{r, fig.width=5, fig.asp=0.7}
A <- igraph::get.adjacency(ukf.und)
desc_A <- eigen(as.matrix(A))
desc_A$values
```

```{r}
vec <- as.numeric(desc_A$vector[,1])
desc_A$values[1]
e_vector <- -vec
qplot(e_vector)
```

```{r}
colores <- colorRampPalette(c('red','green'))
colores_1 <- colores(length(e_vector))
nodos <- data.frame(id=1:length(vec), value = e_vector, 
                    color = colores_1[rank(e_vector)])

visNetwork(nodos, dat.1 %>% select(-weight), 
           width = "100%") %>%
  visPhysics(solver ='forceAtlas2Based', 
             stabilization = TRUE) %>%
  visNodes(value = 1, scaling = list(min = 1, max = 200))
```

Podemos calcular también usando gggraph:

```{r}

uk_tbl <- ukf.und %>% as_tbl_graph() %>%
  activate(nodes) %>%
  mutate(nombre = 1:nrow(as_data_frame(.))) %>%
  mutate(importancia = centrality_eigen())
ggraph(uk_tbl, layout = 'fr') +
  geom_edge_link(alpha=0.2) +
  geom_node_point(aes(size = importancia), colour = 'salmon') +
  geom_node_text(aes(label = nombre), nudge_y = 0.2, size=1) +
  theme_graph(base_family = 'sans')
```



### Ejercicio {#ejemplo}

Usa Gephi para calcular la centralidad de eigenvector/betweeness de tu red de amigos (es no dirigida).

- Bajar tu red: http://snacourse.com/getnet
- Gephi: https://gephi.org




## Gráficas dirigidas

Estos conceptos pueden generalizarse para gráficas dirigidas, cuando hay un concepto de dirección en las relaciones de los nodos.

- In degree, out degree
- Betweenness puede definirse en función de caminos dirigidos.
- Cercanía también (in closeness, out closeness)
- Centralidad de eigenvector: misma idea, pero la matriz $A$ no es simétrica. En este caso, $A_{ij} = 1$ cuando hay una arista que va de $i$ a $j$

Consideremos en particular cómo se calcula la centralidad de eigenvector para una red dirigida.

```{r, fig.width=4}
set.seed(28011)
red.6 <- igraph::erdos.renyi.game(5, p.or.m=0.5, directed=T)
par(mar=c(0,0,0,0))
plot(red.6, vertex.size=40)
```

Su matriz de adyacencia es no simétrica
```{r}
A <- igraph::get.adjacency(red.6)
A
```

```{r}
centralidad <- igraph::evcent(red.6, directed = T)
print(centralidad$vector/sum(centralidad$vector), digits = 2)
```

Para este ejemplo, las ecuaciones de importancia son como sigue. Para el nodo 1,
$$\lambda x_1 = x_2 + x_4 + c_5$$
para el nodo 5, por ejemplo, es
$$\lambda x_5 = x_1 + x_2 + x_3$$
y así sucesivamente.

Obsérvese que en cada ecuación se consideran las aristas *entrantes*, de forma que la ecuación del nodo 1 requiere la columna 1 de la matriz de adyacencia, el nodo 2 la columna 2, etc. Es decir, la ecuación que debemos resolver es

$$A^t x = \lambda x$$

En el ejemplo anterior,
```{r}
v.salida <- as.numeric(eigen(t(as.matrix(A)))$vectors[,1])
print(-v.salida/(sum(-v.salida)), digits = 2 )
```


### Perron Frobenius: 

¿Cuándo podemos garantizar unicidad en la solución de $A^tx=\lambda x$ con $\lambda > 0$?

Sea $A$ la matriz de adyacencia de una gráfica dirigida. Igual que en el caso
de gráficas no dirigidas:

-  Si la gráfica asociada a $A$ es fuertemente conexa (para cualquier par de vértices hay caminos $i\to j$ y $j\to i$) entonces decimos que $A$ es **irreducible**.
- Igualmente, $A$ es irreducible si y sólo si para cualquier $i,j$ existe una
$m>0$ tal que $(A^m)_{i,j}>0$.

Y tenemos el teorema de Perron-Frobenius, de donde concluimos: 
Si $A$ es irreducible, entonces $\lambda_0 > 0$ es un eigenvector simple de A, un vector propio asociado $x$ tiene entradas positivas, y no existe ningún otro vector propio con entradas no negativas que no sea paralelo a $x$

- Podemos definir una medida única de centralidad módulo una constante multiplicativa.


### Ejemplo: $A$ no irreducible {-}

En este caso podemos no tener vectores propios no positivos (algunos
nodos resultan con peso 0), cuando no es fuertemente conexa:

```{r}
red <- igraph::graph(c(1,2,2,3,3,4,4,3), directed = TRUE)
par(mar=c(0,0,0,0)); plot(red, vertex.size=50)
A.red <- as.matrix(igraph::get.adjacency(red))
A.red
eigen(t(A.red))
```

Podemos tener valores propios no simples con distintos
vectores propios no negativos (si la gráfica es disconexa):

```{r}
red <- igraph::graph(c(1,2,2,3,3,1,4,5,5,4), directed = TRUE)
par(mar=c(0,0,0,0)); plot(red, vertex.size = 20)
A.red <- as.matrix(igraph::get.adjacency(red))
A.red
eigen(t(A.red))
```

O ningún valor propio positivo:
```{r}
red <- igraph::graph(c(1,2,2,3), directed = TRUE)
par(mar=c(0,0,0,0)); plot(red, vertex.size=20)
A_red <- as.matrix(igraph::get.adjacency(red))
A_red
print(eigen(t(A.red)), digits=2)
```



## Pagerank

Pagerank es una medida similar a la centralidad de eigenvector:

- La importancia de un nodo (página) es un promedio ponderado de las importancias de otras páginas que apuntan (ligan) hacia ella. 
- Diferencia: usamos **gráficas dirigidas**, y **distribuimos el peso de las aristas dependiendo
de cuántas ligas hacia afuera tiene una página**: es decir, la importancia de una página se diluye entre el número de ligas a sus hijos.

Para una red dirigida de páginas de internet, definimos entonces su **matriz de transición** $M$
como sigue:

- $M_{ij} =1/k$ si la página $i$ tiene $k$ ligas hacia afuera, y una de ellas va al nodo $j$
- $M_{ij} = 0$ en otro caso.

Intentaremos hacer algo similar a la centralidad de eigenvector, pero usando la matriz de transición en lugar de la matriz de adyacencia.

### Ejemplo {-}

```{r, fig.width = 6}
red_p <- igraph::graph(c(1,2,1,4,1,3,2,1,2,4,3,1,4,3,2,3))
plot(red_p, vertex.size=20, edge.curved=T,edge.arrow.size=0.5)
```

La matriz de adyacencia no necesariamente es simétrica, pues la gráfica es dirigida:

```{r}
A <- igraph::get.adjacency(red_p)
A
```


Y calculamos la matriz M, que es la matriz A con los renglones normalizados por
su suma:
```{r}
M <- t(scale(t(as.matrix(A)), center=FALSE, scale=apply(A,1,sum)))
M
```

### La matriz $M$ es estocástica


A la matriz $M$ le llamamos una **matriz estocástica**, pues cada uno
de sus renglones son no negativos y suman 1. Bajo este supuesto, es posible
demostrar otra versión del teorema de Perron Frobenius:

```{block2, type='resumen'}
**Pagerank simple**

Supongamos que $M$ es irreducible, que en el caso dirigido quiere decir que la red
es *fuertemente conexa*: existe un camino *dirigido* entre cualquier par de vértices.

- Entonces el valor propio de Perron-Frobenius (simple) para $M^t$  es $\lambda_0=1$, y el único vector
propio no negativo (módulo longitud) es estrictamente positivo y asociado a $\lambda_0=1$.

Esto implica que si $r$ es el vector de importancias según pagerank, entonces $r$ debe satisfacer 

$$r_j = \sum_{i\to j}  \frac{r_i}{d_i}$$

donde la suma es sobre las ligas de entrada a $j$, y $d_i$ es el grado de salida del nodo $i$ (dividimos la importancia de $r_i$ sobre todas sus aristas de salida). En forma matricial,
$$M^tr = r.$$
```


#### Ejercicio {-}
Escribe las ecuaciones de las dos formas mostradas arriba para la 
red
```{r}
red_p 
```



#### Ejemplo: pagerank simple {-}
```{r, dev='pdf', fig.width=3, fig.height=1}
decomp <- eigen(t(M))
decomp
vec_1 <- as.numeric(decomp$vector[,1])
vec_1
round(vec_1/sum(vec_1), 2)
```

```{r, fig.width=6}
plot(red_p, vertex.size=60*vec_1, 
                          edge.curved=T, edge.arrow.size=0.5)
```

¿Por qué 1 tiene mayor importancia que 3? Nótese que el nodo 3 tiene mayor grado que 1. La razón de
que el pagerank de 1 es mayor que el de 3 es que la importancia
del nodo 1 se diluye pues tiene grado 3 de salida, mientras
que toda la importancia de 3 se comunica al nodo 1.


### Primeras dificultades

¿Qué puede fallar cuando queremos encontrar el pagerank? 
 
- Si existen **callejones sin salida** la matriz $M$ no es estocástica, pues tiene un renglón de ceros - no podemos aplicar la teoría de arriba. Por ejemplo, la siguiente matriz no tiene un valor propio igual a 1 (conclusión invalidada: hay un valor propio igual a 1):

```{r}
red.p <- igraph::graph(c(1,2,2,1,1,3))
par(mar=c(0,0,0,0)); plot(red.p, vertex.size=30, edge.curved=T,edge.arrow.size=0.5)
A <- igraph::get.adjacency(red.p)
M <- t(scale(t(as.matrix(A)), center=FALSE, scale=apply(A,1,sum)))
M[3, ]  <- 0
eigen(t(M))
```

- Si existen **trampas de telaraña** (spider traps) entonces la matriz $M$ es estocástica, pero las soluciones concentran todo la importancia en la trampa (en este caso consiste de los nodos 1 y 2) (conclusión invalidada: el vector de importancias es positivo).

```{r}
red.p <- igraph::graph(c(1,2,2,1,3,1,3,4,4,5,5,3))
par(mar=c(0,0,0,0)); plot(red.p, vertex.size=30, edge.curved=T,edge.arrow.size=0.5)
A <- igraph::get.adjacency(red.p)
M <- t(scale(t(as.matrix(A)), center=FALSE, scale=apply(A,1,sum)))
eigen(t(M))
```

- La red puede ser **disconexa**.  Por ejemplo, cuando hay dos componentes irreducibles, existe más de un vector propio asociado al valor propio 1 (conclusión invalidada: la solución es única), así que hay tantas soluciones como combinaciones lineales de los eigenvectores que aparecen:

```{r}
red.p <- igraph::graph(c(1,2,2,3,3,1,4,5,5,6,6,5,6,4))
par(mar=c(0,0,0,0)); plot(red.p, vertex.size=30, edge.curved=T,edge.arrow.size=0.5)
A <- igraph::get.adjacency(red.p) %>% as.matrix
M <- t(scale(t(A), center=FALSE, scale=apply(A,1,sum)))
M
sol <- eigen(t(M))
round(sol$values,2)
vec.1 <- sol$vectors[,3]
vec.2 <- sol$vectors[,4]
vec.1
vec.2
```


En términos de nuestra solución para dar importancia de páginas:

- No es razonable que nuestra solución concentre toda la importancia en spider traps.
- Si la gráfica es disconexa no podemos dar importancia relativa a las componentes resultantes
- Si hay callejones sin salida entonces nuestra formulación no sirve.

---


Para encontrar una solución, podemos pensar en el proceso estocástico asociado a esta formulación de pagerank.

### El proceso estocástico (cadena de Markov) asociado al Pagerank, versión simple

Podemos interpretar este proceso mediante una cadena de Markov. 
Consideramos una persona que navega al azar en nuestra red (haciendo click
en las ligas disponibles en cada nodo):

- Comienza en una página tomada al azar (equiprobable).
- Cada vez que llega a una página, escoge al azar alguna de las páginas ligadas en su página actual y navega hacia ella.
- **Suponemos por el momento que no hay callejones sin salida**.

Denotamos por $X_1, X_2,\ldots$ la posición del navegador en cada momento del tiempo. Cada $X_i$ es una variable aleatoria que toma valores en los nodos $\{1,2,\ldots,n\}$.


- $X_1, X_2,\ldots$ es un proceso estocástico discreto en tiempo discreto.
- Para determinar un proceso estocástico, debemos dar la distribución
conjunta de cualquier subconjunto $X_{s_1},X_{s_2},\ldots, X_{s_k}$ de variables. En este
caso, la posición en $s+1$ sólo depende de la posición en el momento $s$, de forma
que basta con especificar
$$P(X_{s+1}=j\vert X_s=i), $$
para cada par de páginas $i$ y $j$. 

### Matriz de transición

Ahora podemos ver que 

- Si hay una liga de $i$ a $j$, entonces $P_{ij}=1/k(i)$, donde $k(i)$ es el número
de ligas salientes de $i$.
- Si no  hay liga de $i$ a $j$, entonces $P_{ij}=0$.

Es claro que $P$ es igual a la matriz $M$ que definimos con anterioridad. 


### Distribución de equilibrio.

La matriz $P$ es estocástica. Si suponemos que $P$ es irreducible (la gráfica es fuertemente conexa), entonces por la teoría que vimos arriba
existe un vector $\pi > 0$ tal que $P^t\pi = \pi$.

- En términos del modelo del navegador aleatorio, ¿ qué
significa entonces que un vector $\pi$ satisfaga $P^t \pi = \pi$, con $\pi \geq 0$?

Suponemos $\pi$ normalizado por la suma: $\sum_i \pi_i=1$.

- Significa que si escogemos un estado al azar con probabilidad $\pi$, entonces, después de un salto, las probabilidades de encontrar al navegador en cada estado está dado también por $\pi$. 
- Igualmente, la probabilidad de encontrar al navegador en cualquier momento en el estado $i$ es igual a $\pi_i$.

### Distribución de equilibrio y probabilidades a largo plazo.

Sin embargo, en un principio no conocemos la distribución de
equilibrio $\pi$. Lo que haríamos sería escoger un nodo al azar y comenzar
a navegar desde ahí, o quizá empezaríamos en un nodo fijo, por ejemplo, el 1.

Una pregunta más interesante es entonces, 

- Si el navegador pasa navegando una cantidad grande de tiempo,
¿cuál es la probabilidad de encontrarlo en un momento dado en 
el nodo $i$?

La respuesta es que solamente con el supuesto de conexidad fuerte  no podemos contestar esta pregunta de manera simple.

#### Ejemplo {-}

```{r}
red.p <- igraph::graph(c(1,2,2,3,3,1))
par(mar=c(0,0,0,0)); plot(red.p, vertex.size=30, edge.curved=T,edge.arrow.size=0.5)
A <- igraph::get.adjacency(red.p)
M <- t(scale(t(as.matrix(A)), center=FALSE, scale=apply(A,1,sum)))
M
```

Y notamos que si comenzamos en el estado uno, la probabilidad de estar en cada estado al tiempo 2 es
```{r}
t(M) %*% c(1,0,0)
```
En el tiempo 3:
```{r}
t(M) %*% t(M) %*% c(1,0,0)
```
En el tiempo 3:
```{r}
t(M) %*% t(M) %*% t(M) %*% c(1,0,0)
```
y así sucesivamente, de manera que las probabilidades de visita a cada nodo dependen del tiempo.



Recordamos que la matriz $P^k$ da las probabilidades de transición
a $k$ pasos. 

Por ejemplo, para $k=2$ tenemos la probabiildad de pasar de $i$ a $j$ en dos pasos es igual a (probabilidad total):

$$P(X_3=j|X_1=i) =\sum_k P(X_3=j|X_2=k, X_1=i) P(X_2=k|X_1=i)$$

Por la propiedad de Markov, podemos simplificar

$$P(X_3=j|X_1=i) =\sum_k P(X_3=j|X_2=k) P(X_2=k|X_1=i)$$

y si sustituimos $P$

$$P(X_3=j|X_1=i) =\sum_k P_{i,k} P_{k,j}. $$

Como el lado derecho es la componente $i,j$ de $P^2$, tenemos que
$P^2$ da las probabilidades de transición en 2 pasos.

Como ejercicio, calcula e interpreta $P^2$ para el ejemplo anterior.



### Cadenas aperiódicas
Decimos que $P$ irreducible es aperiódica cuando
$P^{k_0}>0$ para alguna $k_0$ suficientemente grande
###

Nótese que en el ejemplo anterior $P$ no es aperiódica. Con esta condición (no importa en qué estado estamos al tiempo $k_0$ el navegador puede estar en cualquier estado), podemos dar una respuesta más simple a la pregunta acerca de las probabilidades de largo plazo de la cadena:

### Probabilidades a largo plazo {#importante}
Si $P$ es una matriz irreducible aperiódica, y $\pi$ es su distribución de equilibrio, entonces 
$$\lim_{n\to \infty} P(X_n=i) = \pi_i,$$
independientemente de la distribución inicial.



Otra manera de ver esto es como sigue:

Sea $v$ una distribución inicial sobre los estados (cualquiera). Entonces, 
si $P$ es irreducible y aperiódica,
$$(P^n)^t v \to \pi$$
cuando $n\to \infty$.






Si $P^t \pi=\pi$, con $\pi \geq 0$, $\sum_i \pi_i=1$, decimos que $\pi$ es
una **distribución de equilibrio** para la cadena de Markov.

Si escogemos el estado inicial para el navegador aleatorio con probabilidades
$\pi$ sobre las páginas, entonces en cualquier momento del tiempo la probabilidad
de encontrar al navegador en la página $i$ es $\pi_i$. Esto es porque:

- $P^k$ da las probabilidades de transición a $k$ pasos
- $\pi P^k=\pi$ da las probabilidades de encontrar al proceso en el cada estado,
dado que $P(X_1=i)=\pi_i$.

De modo que si escogemos el estado inicial con distribución $\pi$, $\pi$ también
describe qué tan frecuente es encontrar al navegador en cada estado para cualquier tiempo.

Si la matriz es irreducible, entonces siempre existe una distribución de equilibrio.

### Matriz de transición: P es primitiva


Si adicionalmente existe una $k$ tal que $P^k$ es una matriz positiva (sin entradas
igual a cero), decimos que $P$ es **primitiva**, y podemos concluir algo más fuerte:

- *No importa con que probabilidades se escoja el estado inicial*, a largo plazo,
la probabilidad de encontrar al proceso en el estado $i$ es igual a $\pi_i$.

Ejemplo: este supuesto evita periodicidad: supongamos que hay dos páginas, cada una con una liga a la otra. La cadena es irreducible, pero si el navegador empieza en la página
1, entonces sólo puede caer en 1 en tiempos impares. De modo que el inciso de 
arriba no puede ser cierto.

Observación: para cadenas con un número de estados infinito no es tan simple!

### Matriz primitiva {#importante}
Cuando la matriz de transición $P$ es primitiva, y $\pi$ es la distribución estacionaria, entonces
$$(P^k)^t x \to \pi$$
para cualquier $x$ inicial.

### 

Nótese que aplicando a $x=e:j$ esto implica que $P^k$ converge a una matriz cuadrada que
tiene todas sus columnas igual a $\pi$, la distribución de equilibrio.

### Problemas: si no es fuertemente conexa

Como vimos antes, si la matriz no es irreducible, entonces

1. Puede haber callejones sin salida (páginas o estados *absorbentes*), en cuyo caso $M$ ni siquiera está definida 
(o se puede definir pero no es estocástica).

2. Puede haber *spider-traps*: en este caso, se trata de un conjunto de 
nodos *absorbente*: una vez que se entra a este conjunto no es posible salir. La solución
asignará peso positivo a los elementos del *spider-trap* y cero al resto de la red.

3. Puede haber distintas componentes fuertemente conexas, y entonces no hay una única solución.

4. En los casos 1, 2 y 3, y también si la matriz no es primitiva, puede ser
que la frecuencia de visita a largo plazo de las páginas dependa del estado inicial donde
comenzó el navegador.

### Problemas: *spider traps*

En este ejemplo, 1,2,3 y 4 forman un *spider-trap*, pues una
vez que se entra a esta componente no es posible salir.

```{r, fig.width=6}
red.p.2 <- igraph::graph(c(1,2,1,4,1,3,2,1,2,4,3,1,4,3,5,4,5,6,6,7,7,5))
par(mar=c(0,0,0,0)); plot(red.p.2, edge.arrow.size=0.2, vertex.size=20)
round(igraph::page.rank(red.p.2, damping=1)$vector,3)
```



### Problemas

Y si hay varias componentes fuertemente conexas, entonces
la solución no es única.

```{r, fig.width=6}
red.p.2 <- igraph::graph(c(1,2,1,4,1,3,2,1,2,4,3,1,4,3,5,4,6,7,7,6))
par(mar=c(0,0,0,0)); plot(red.p.2, edge.arrow.size=0.5)
```

```{r}
A <- igraph::get.adjacency(red.p.2)
M <- t(scale(t(as.matrix(A)), center=FALSE, scale=apply(A,1,sum)))
eigen(t(M))
```


### Solución: perturbación de la matriz $M$.


Podemos resolver estos problemas perturbando ligeramente la matriz $M$:
si $e$ es un vector de unos, $ee^t$ es una matriz cuadrada que contiene
unos. Perturbamos entonces tomando:
$$M_1  = \alpha M + (1-\alpha)ee^t/n$$
con $\alpha$ cercana a 1.

- Como la matriz $M_1$ es positiva, existen una única solución de Perron
Frobenius con vector propio positivo (asigna importancia mayor a  0 a todos los nodos)



Tomando $\alpha=0.85$ obtenemos por ejemplo para nuestra matriz $M$ anterior:

```{r}
A <- igraph::get.adjacency(red.p.2)
M <- t(scale(t(as.matrix(A)), center=FALSE, scale=apply(A,1,sum)))
unos <- rep(1,nrow(M))
alpha <- 0.85
M.1 <- alpha*M + (1-alpha)*unos%*%t(unos)/nrow(M)
M.1[1:3,]
```



```{r}
eigen(t(M.1))
```


```{r, dev='pdf', fig.width=4, fig.height=2}
descomp <- eigen(t(M.1))
pesos.pr <- as.numeric(eigen(t(M.1))$vectors[,1])
par(mar=c(0,0,0,0))
plot(red.p.2, vertex.size = 50*pesos.pr, edge.arrow.size = 0.5)
```



La perturbación también podemos interpretarla desde el punto de vista
del *navegador aleatorio*: 

- Comienza en una página tomada al azar (equiprobable).
- Cada vez que llega a una página, con probabilidad $1-\alpha$ chica se teletransporta a una página tomada al azar. Con probabilidad $\alpha$ escoge una liga al azar de su página actual y navega hacia ella. 

Usando probabilidad total, vemos entonces que

- Si hay una liga de $i$ a $j$, entonces $P_{ij}= \frac{(1-\alpha)}{n} + \alpha/k(i)$ :
la probabilidad de ir de $i$ a $j$ es la suma de:
    1. La probabilidad de teletransportarse ($1-\alpha$) por la probabilidad de caer al azar en $j$ dado que se teletransportó ($1/n$).
    2. La probabilidad de no teletransportarse ($\alpha$), yluego  escoger la liga de $i$ a $j$ ($1/k$).

- Si no hay una liga de $i$ a $j$, entonces $P_{ij}= \frac{(1-\alpha)}{n}$, pues
sólo puede ir de $i$ a $j$ teletransportándose.

### Matriz de transición y su perturbación

Es fácil ver entonces que  en este caso la matriz de transición para el proceso
del navegador aleatorio con teletransportación está dada por la matriz perturbada 
que definimos anteriormente:

$$M_1  = \alpha M + (1-\alpha)ee^t/n$$

- La perturbación actúa como una especie de regularización que garantiza
una solución única de Perron Frobenius.
- $\alpha$ se escoge normalmente en el rango de 0.8 a 0.9.


- Con esta perturbación , la cadena es irreducible y aperiódica. Los spider-traps
desaparecen, no hay callejones sin salida, sólo hay una componente fuertemente conexa.
- Podemos interpretar la distribución de equilibrio como las frecuencias de visita
a las páginas, independientemente de dónde comenzó el navegador.

