# Algoritmos de tragamonedas (bandits)

En esta sección consideramos el problema de hacer cambios
en un sitio web, por ejemplo, con el fin de mejorar métricas
como conversiones, ventas, tráfico, tasas de click-through, etc.


## Causalidad y contrafactuales

Para entender por qué este problema no es trivial, consideremos
que alguien estamos considerenda hacer una modificación del algoritmo de búsquedas de una tienda online. Por ejemplo: quizá estamos pensando
en usar un nuevo algoritmo para ordenar los resultados de una búsqueda
en la tienda.

En algunos casos es claro que nuestro cambio representa una mejora (por ejemplo, cuando arreglamos un *bug*), pero muchas veces no estamos muy
seguros de que el cambio vaya a ser beneficioso. En este ejemplo,
podriamos tomar como métrica de interés el número de conversiones: cuántas de las búsquedas terminan en acciones positivas (compras, añadir a canasta, etc.)

Una primera aproximación es hacer el cambio, y observar las métricas de interés. Sin embargo, los números resultantes **pueden ser difíciles de interpretar**: por ejemplo, si hicimos el cambio justo antes de navidad, es
normal esperar más compras resultantes de las búsquedas, pues en promedio
la gente que llega está más decidida a comprar que en otros periodos.
Esto no quiere decir que nuestro cambio fue positivo, pues quizá las 
ventas hubieran sido aún mayores si no hubiérmos hecho el cambio.

Otra aproximación simplista en los negocios es comparar *year over year*, por ejemplo, comparamos esta navidad con las anteriores. Esto también da un
resultado difícil de interpretar, pues estamos comparados dos periodos
alejados en el tiempo.

```{block2, type="resumen"}
Desde el punto de vista conceptual, lo que quisiéramos saber es *cómo
nos hubiera ido* si no hubiéramos hecho el cambio. Si supiéramos este
resultado, podríamos comparar directamente y evaluar el efecto de nuestro
cambio. A este escenario típicamente no observado le llamamos **contrafactual**, y es útil para razonar en este tipo de problemas.
```

#### Ejemplo {-}

En este ejemplo consideramos un escenario posible (simulado)
```{r, message=FALSE, warning = FALSE}
if(!require(CausalImpact)){
  install.packages("CausalImpact")
}
library(CausalImpact)
library(tidyverse)
set.seed(4)
x1 <- 100 + arima.sim(model = list(ar = 0.999), n = 100)
y <- 1.2 * x1 + rnorm(100) # ventas simuladas
y[71:100] <- y[71:100] + 7 # efecto de la intervención en el periodo 70.
time.points <- seq.Date(as.Date("2014-01-01"), by = 1, length.out = 100)
data <- zoo(cbind(y, x1), time.points)
pre.period <- as.Date(c("2014-01-01", "2014-03-11"))
post.period <- as.Date(c("2014-03-12", "2014-04-10"))
impact <- CausalImpact(data, pre.period, post.period)
plot(impact, metrics="original")
```


La línea sólida son las ventas observadas (diarias). La línea vertical señala
cuando se hizo la intervención. La línea punteada es el contrafactual estimado: cuáles hubieran sido las ventas si no hubiéramos hecho la intervención. Nótese que un análisis superficial que sólo considera las ventas observadas podría concluir que la intervención no tuvo efecto, pero en realidad la situación es muy diferente: la intervención sostuvo las ventas durante el mes analizado.



- Este análisis se basa en la construcción de **contrafactuales** con modelos dinámicos bayesianos. La idea es simple: supongamos que nuestra métrica de interés es $y$. Enconcontramos una covariable $x_1$, y construimos un modelo
(con los datos anteriores a la intervención) para $y$ con $x_1$ como covariable. La covariable $x_1$ debe ser escogida de forma que **tengamos evidencia fuerta** de que esta variable **no será afectada por la intervención**. De este forma, el contrafactual después de la intervención se construye haciendo la predicción de $y$ basada en $x_1$, y nos da una idea de lo que hubiera pasado sin la intervención.

- Por ejemplo, en algunos casos podemos escoger $x_1$ como las ventas de la región A, $y$ son la ventas de la región $B$. Nuestra intervención (por ejemplo, una promoción) la aplicamos solamente en la región $B$. Entonces el modelo basado de $y$ en $x_1$ (ventas región A, que no es intervenida) nos da el contrafactual para comparar.

Estas ideas son de el 
método propuesto en [CausalImpact](https://github.com/google/CausalImpact), 
[@causalimpact]. Se puede ver como una versión más sofisticada y poderosa 
del método de diferencia de diferencias.

## Experimentos A/B

Cuando consideramos cambios a un servicio web, y queremos entender el 
efecto en los usuarios, podemos considerar otra estrategia para 
estimar el efecto de nuestras intervenciones.

Considerando que tenemos un número relativamente grande de visitas,
podemos hacer un **experimento A/B**:
seleccionamos al azar un subconjunto de usuarios para mostrarles

- el sitio **con** la intervención (grupo B), y al resto les mostramos
- el sitio **sin** la intervención (grupo A). 

Como seleccionamos al azar
los usuarios, las poblaciones son comparables en cuanto a demográficos y
tipos de usuarios, y no tenemos problemas de hacer comparaciones fuera de temporalidad. Comparamos los resultados de los dos grupos
para evaluar el efecto de la intervención.

Nótese que esta estrategia puede no funcionar bien si usamos una asignación
a grupos que no es aleatoria. Un ejemplo típico es considerar el efecto
de un anuncio de *paid search*. Podemos comparar usuarios que recibieron
el anuncio con otros que no lo recibieron, y ver las ventas en cada grupo. Esta comparación típicamente *no* resulta en una buena estimación 
de la cantidad que nos interesa, pues los que recibieron el anuncio
de *paid search* han mostrado más interés en hacer una compra que los que
no lo recibieron (por eso recibieron el anuncio!)

#### Ejemplo {-}

Veamos un ejercicio de simulación. Consideremos dos grupos de usuarios:
interesados y no interesados, que es una variable que no observamos. 
Suponemos que las ventas en cada grupo, independientemente de nuestras
acciones, son más altas para usuarios interesados que no interesados:

```{r}
compras_int_alto <- rpois(1000, 5)
compras_int_bajo <- rpois(1000, 1)
mean(compras_int_alto)
mean(compras_int_bajo)
```

Nótese que nosotros en principio no podemos hacer esta comparación,
pues *interés* no es una medida que tengamos disponible fácilmente.


Ahora supongamos ahora que las probabilidades de recibir el anuncio de *paid search* es más alto para los interesados que los no interesados (pues usuarios interesados hacen más búsquedas relevantes):

```{r}
ps_int_alto <- 0.04
ps_int_bajo <- 0.01
```

Y simulemos los datos que observaríamos. En primer lugar,
simulamos qué usuarios reciben o no anuncios:

```{r}
anuncio_int_alto <- rbinom(1000, 1, ps_int_alto) 
anuncio_int_bajo <-  rbinom(1000, 1, ps_int_bajo)
head(anuncio_int_alto)
```

Y juntamos los datos que observaríamos:

```{r}
datos <- data_frame(anuncio_ps= c(anuncio_int_alto, anuncio_int_bajo),
                    compras = c(compras_int_alto, compras_int_bajo))
datos
```


Si hacemos una comparación entre grupos, obtenemos:

```{r}
datos %>% group_by(anuncio_ps) %>% 
  summarise(media_compras = round(mean(compras), 2), n = n())
```
Y vemos que el grupo que recibió anuncio tiene más compras promedio
que el que no recibió anuncio. Sin embargo, en esta simulación
el anuncio no tiene ningún efecto.

Sin embargo, si distribuimos al azar la asignación del anuncio,

```{r}
datos %>% mutate(asigna = rbinom(2000, 1, 0.05)) %>% 
  group_by(asigna) %>% summarise(media_compras = round(mean(compras), 2), n = n())
```

Entonces obtenemos una comparación de "peras con peras"

### Aleatorización en experimentos.

Aunque intuitivamente está claro por qué en el ejemplo anterior podemos
obtener el análisis correcto cuando aleatorizamos el tratamiento (en este caso un anuncio), podemos ver algunos detalles matemáticos que explican por qué sucede.

Una manera fácil de entender esto es pensando en contrafactuales. Consideramos
las siguientes variables aleatorias:

- $Y (1)$ = compras de usario si recibe anuncio.
- $Y (0)$ = compras de usuario si no recibe anuncio.

Lo que típicamente nos interesa saber es
\begin{equation}
E\left[ Y(1)-  Y(0)\right ] = E[Y(1)]  - E[Y(0)]
(\#eq:difcausal)
\end{equation}

que podemos considerar como el efecto promedio del tratamiento (un anuncio, un cambio en el sitio, etc) sobre los usuarios.

En la práctica, consideramos una muestra de usuarios $Y_1,Y_2,\ldots, Y_n$. Algunos de estos recibirán el tratamiento y otros no, y escribimos $T_i=1$ si $i$ recibió el tratamiento y $T_i=0$ si no lo recibió. Las variables $Y_i$ que observamos
son:

- $Y_i = Y_i(1)$ si $T_i=1$
- $Y_i = Y_i(0)$ si $T_i=0$

La estimación directa de el valor esperado \@ref(eq:difcausal) la haríamos con
el promedio
$$\frac{1}{n}\sum_i (Y_i(1) - Y_i(0))$$
y notamos inmediatamente una dificultad: no observamos esta diferencia
para ninguno de los usuarios! Para cada usuario, observamos solamente una
de $Y_i(1)$ o $Y_i(0)$. Los datos, por ejemplo, se verían:

```{r}
datos_obs <- data_frame(i = c(1L,2L,3L,4L,5L), 
           `Y_i(1)` = c(NA, 5L, 3L, NA, NA),
           `Y_i(0)` = c(2L, NA, NA, 3L, 2L), 
            T=c(0L,1L,1L,0L,0L),
            Y_i=c(2L,5L,3L,3L,2L))
datos_obs
```

En cada caso, necesitamos el **contrafactual** de cada observación para poder
estimar el efecto promedio de nuestra intervención.

Nótese que viendo el lado derecho de \@ref(eq:probacola) podríamos intentar
hacer la *estimación estándar*, que consiste en tomar los promedios
de las columnas de los datos anteriores y promediar:
\begin{equation}
 \frac{1}{n_1} \sum_{T_i = 1} Y_i - \frac{1}{n_0} \sum_{T_i=0} Y_i.
(\#eq:estandar)
\end{equation}

donde promediamos los resultados de los tratados y restamos el promedio
de los no tratados.  En nuestro ejemplo, haríamos:

```{r}
mean.na <- function(x){ mean(x, na.rm = T)}
datos_obs %>%  
  summarise(media_tratados = mean.na(`Y_i(1)`),
            media_no_tratados = mean.na(`Y_i(0)`)) %>%
  mutate(diferencia = media_tratados - media_no_tratados)
```

Ahora consideremos por qué esta estimación puede tener sesgo alto.
El promedio $\frac{1}{n_1} \sum_{T_i = 1} Y_i$ es realmente una estimación
de 
$$E[Y|T = 1] = E[Y(1)|T=1]$$
Así que cuando la asignación del tratamiento está asociada a la variable
de respuesta $Y$, esta última cantidad no es necesariamente igual
a $E(Y(1))$. Explica en tus palabras por qué pasa esto (puedes comenzar
con el ejemplo de *paid search* que vimos arriba).

Igualmente, el promedio $\frac{1}{n_0} \sum_{T_i = 0} Y_i$ estima
la cantidad
$$E[Y|T = 0] = E[Y(0)|T=0]$$
que no necesariamente es igual a $E[Y(0)]$.

```{block2, type="resumen"}
El que a veces se llama el **problema fundamental de la inferencia causal** es
que nunca observamos los efectos causales directamente, pues nunca observamos
juntos el resultado más su contrafactual.
```


Con un experimento, sin embargo, podemos superar este problema.
Si **asignamos el tratamiento de manera aleatoria**, entonces
por definición $E(Y(0)) = E(Y(0)|T=0)$ y $E(Y(1)) = E(Y(1)|T=0)$ (T
es una variable aleatoria que no está correlacionada con ninguna otra variable),
y el estimador estándar \@ref(eq:estandar) es un estimador insesgado 
de la cantidad que nos interesa, que es \@ref(eq:difcausal).



## Costos de experimentación

Ahora supongamos que para simplificar nuestra vida, y tener resultados
sólidos, decidimos hacer un experimento A/B en nuestro sitio web. Decidimos
entonces, cada vez que un usuario pide una página x, tiramos un volado y decidimos
si le enseñamos la versión A y la versión B. Lo que hace este usuario en el resto
de su sesión estará registrado en los logs del sitio (o en una base de datos si
nuestra aplicación registra los resultados de interés). Podemos entonces, después
de tener un número apriopiado de usuarios en cada rama A y B, comparar los
dos grupos y tomar una decisión final acerca de qué versión nos conviene.

Las primeras objeciones que podemos hacer a esta decisión (y que muchos interesados
en el negocio seguramente plantearán):

- El experimento representa un riesgo al ingreso del sitio o el éxito del negocio. Si la modificación es contraproducente, entonces la duración del experimento representa pérdidas para el negocio. Por ejemplo, si decidimos hacer un experimento para saber si un anuncio está funcionando, en lugar de mostrar el anuncio a todos los usuarios que califican, solo una fracción lo recibirá. Esto representa pérdidas potenciales grandes. **Respuesta**: esto es cierto, en términos de ganancias a corto plazo.  Pero también es necesario ver el *trade-off*: que a mediano y largo plazo, hacer experimentación de manera más generalizada puede avanzar mejor el negocio y dar más seguridad en la toma de decisiones.

- Hay también un costo técnico en establecer la infraestructura para hacer y analizar los experimentos adecuadamente.  **Respuesta**: esto es cierto.

- El camino de la experimentación muchas veces es contrario al pensamiento de los analistas de negocios/tomadores de decisiones, que a veces prefieren hacer análisis diseñados para entender el potencial de cambio *antes* de hacer ningún cambio. Por ejemplo encuestas, pruebas en focus groups, etc. **Respuesta**: el análisis ad-hoc es una herramienta poderosa, pero también muchas veces plagada de incertidumbre no siempre explícita (por ejemplo, muy dependiente del tipo de modelos que se consideran, o de los datos que se tienen a mano). Es posible combinar las dos aproximaciones.

En particular, los experimentos A/B (que pueden tener más de 2 opciones) tienen la debilidad de que

- Cuando hacemos experimentación, generalmente decidimos cuánto tiempo va a durar el experimento para tener tamaños de muestra adecuados. En algunos casos, las opciones son tan malas o tan buenas en relación a otras que esto es un desperdicio de dinero (podemos descartar una opción rápidamente por ser muy mala). 

- Desde el punto de vista del negocio, preferiríamos una solución donde cuanto más mala parece ser una opción, la exploramos menos para incurrir en menos pérdidas grandes. Sin embargo, queremos mantener algún nivel de exploración para poder encontrar mejores opciones a la que tenemos actualmente, especialmente cuando tenemos incertidumbre alta acerca de una opción.

## Algoritmos de tragamonedas

Los algoritmos de tragamonedas (multi-armed bandit algorithms) buscan 
dar soluciones a las deficiencias que acabamos de señalar en el esquema
de experimentación A/B. La idea básica es que para optimizar nuestro
sitio web (por ejemplo), necesitamos balancear dos tipos de acciones:

- **Exploratorias**: donde probamos nuevas opciones para ver cómo se desempeñan.
- **Explotadoras**: donde aplicamos las mejores opciones para tener buenos resultados de negocio.

Estas dos caras son indispensables para la optimización: no tiene mucho sentido
hacer ninguna de ellas si la otra no está presente (no queremos explotar opciones malas, ya sea porque no hemos explorado opciones, o porque desperdiciamos demasiado tiempo en explorar otras opciones).

```{block2, type="resumen"}
Una tragamonedas de $n$ brazos es un proceso aleatorio, donde:

- Escogemos uno de $n$ palancas de la tragamonedas
- Obtenemos una recompensa aleatoria, cuya distribución depende de la palanca que escogimos.

¿Cómo debemos operar la máquina para lograr maximizar en algún
sentido nuestra recompensa?
```


#### Ejemplos {-}
Probemos como lucirían los datos obtenidos de una tragamonedas, y cómo podríamos
ir tomando decisiones acerca de qué brazos probar

```{r}
crear_maquina_poisson <- function(lambda){
  # utilizamos recompensas poisson con distintas medias lambda
  n_brazos <- length(lambda)
  simular_maquina <- function(brazo){
      rpois(length(brazo), lambda = lambda[brazo])
  }
  simular_maquina
}
sim_1 <- crear_maquina_poisson(lambda = c(1, 5, 8))
```

Ahora empezamos a explorar:

```{r}
set.seed(2123)
#jalamos el brazo 2
sim_1(2)
# probemos otro
sim_1(3)
# el brazo 2 parece mejor, pero estamos seguros? Probemos más
sim_1(2)
sim_1(3)
# quizá el brazo 3 es mejor
sim_1(2)
sim_1(3)
```

```{r}
sim_1(3)
```

Pero ahora nos damos cuenta: quizá el brazo 1 es mucho mejor y hemos estado perdiendo
el tiempo en los brazos 2 y 3!

```{r}
sim_1(1)
sim_1(1)
sim_1(1)

```

Parecen mejor los brazos 2 y 3. Veamos un resumen del desempeño :

```{r}
brazo_2 <- c(10, 3, 7)
brazo_3 <- c(6, 13, 11, 3)
mean(brazo_2)
mean(brazo_3)
```

Y nos damos cuenta de que todavía tenemos que explorar más, aunque los brazos 2 y 3 se ven
mejores que el 1.



---

Puntos importantes:

- Las datos que hemos visto de una palanca dada son evidencia del desempeño de la palanca,
pero no podemos decidir con 100\% de seguridad cuál es su desempeño. 
- En general, queremos **explorar** palancas que tengan poca evidencia (datos) para entender
mejor su nivel de recompensas y ver si son buenas.
- Pero también tenemos que **explotar** palancas cuya evidencia es de recompensa alta para no 
perder dinero en el proceso de expermentación. Queremos también evitar palancas que han dado
evidencia de ser inferiores.
- Un esquema de **exploración-explotación** es un punto medio entre la visión del experimentador
clásico y de los intereses del negocio.
- En un experimento clásico (A/B), generalmente fijamos el tamaño de muestra que queremos para hacer
nuestro análisis estadístico. Pero en una tragamonedas,
la asignación es dinámica, y puede depender tanto
del tamaño de muestra como de los niveles de recompensa que hemos observado.


```{block2, type='resumen'}
También obsérvese que en cualquier caso, un algoritmo para resolver una tragamonedas
debe **involucrar aleatorización**: asignación aleatoria de los usuarios (o las pruebas)
a los distintos posibles brazos. Esto nos permite comparar directamente los resultados sin
preocuparnos por corregir o considerar sesgo debido a asignaciones no aleatorias de los
usuarios o las pruebas.
```

En optimización de sitios web, las palancas son las distintas opciones
que podemos presentar a los usuarios, por ejemplo, 3 algoritmos de 
ordenación de resultados. Cada usuario es una jugada en la máquina
tragamonedas, y la recompensa es la métrica de desempeño que hayamos escogido
(por ejemplo, conversiones). 



## Algoritmo epsilon-miope

Idea: casi siempre escoger la que parece ser la mejor opción (media más alta, por ejemplo), 
con los datos que tenemos al momento. Pero de vez en cuando (con cierta
probabilidad) escogemos una opción al azar.

Sea $\epsilon > 0$, y supongamos que nuestra máquina tiene $n$
brazos. Cuando llega un usuario,

- Con probabilidad $1-\epsilon$ le presentamos la mejor opción que tenemos ahora
- Con probabilidad $\epsilon$ es escogemos al azar alguna de las $n$ opciones.


Antes de seguir, probamos el simulador que construimos 
arriba y gráficamos los resultados para cada brazo:

```{r, fig.height = 3}
simular <- crear_maquina_poisson(lambda = c(1, 8, 10))
sim_recomp <- 
  data_frame(brazo = sample.int(3, 1000, replace = TRUE)) %>% 
  mutate(recompensa = simular(brazo)) %>%
  group_by(brazo) %>% mutate(media = mean(recompensa))
ggplot(sim_recomp, aes(x=recompensa)) + facet_wrap(~brazo) +
  geom_histogram(binwidth = 1.2) +
  geom_vline(aes(xintercept = media), colour = 'gray')
```


Ahora construimos nuestra función para el algoritmo e-miope

```{r}
crear_epsilon_miope <- function(epsilon, inicial = 1, sim_fun){
  n_brazos <- environment(sim_fun)$n_brazos
  conteos <- rep(0, n_brazos)
  iteracion <- 0
  #recompensas <- vector("list", n_brazos)
  sumas <- rep(0, n_brazos)
  mejor <- inicial
  epsilon <- epsilon
  fun <- function(){
    if(runif(1) <= epsilon){
      #explorar
      brazo <- sample.int(n_brazos, 1)
    } else {
      #explotar
      brazo <- mejor
    }
    sim <- sim_fun(brazo)
    #recompensas[[brazo]] <<- c(recompensas[[brazo]], sim)
    conteos[brazo] <<- conteos[brazo] + 1
    sumas[brazo] <<- sumas[brazo] + sim
    mejor <<- which.max(sumas /conteos)
    iteracion <<- iteracion + 1
    estado <- data_frame(n = iteracion,
                         brazo = 1:n_brazos,
                         conteos = conteos,
                         sumas = sumas)
    return(estado)
  }
  fun
}
```


Y veamos algunas corridas:

```{r}
set.seed(9233)
e_miope <- crear_epsilon_miope(epsilon = 0.5, inicial = 1, sim_fun = simular)
e_miope()
e_miope()
```

Y ahora generamos 100 datos adicionales y guardamos los resultados:

```{r}
e_miope <- crear_epsilon_miope(epsilon = 0.3, inicial = 1, sim_fun = simular)
df_iteraciones <- lapply(1:300, function(i){
  e_miope()
}) %>% bind_rows %>% as_tibble()
df_iteraciones 
```

Y consideramos la recompensa promedio acumulada a cada momento
del tiempo:
```{r}
resumen <- df_iteraciones %>% group_by(n) %>%
  summarise(promedio_recompensa = sum(sumas) / sum(conteos))
ggplot(resumen, aes(x=n, y= promedio_recompensa)) + geom_line() +
  geom_hline(yintercept = 10, colour = "red")
```

Nótese que esperamos que conforme pasa el tiempo, vamos explorando
las distintas opciones, y el algoritmo identifica
correctamente la mejor opción (la que tiene promedio más alto). Este balance
hace que típicamente  la recompensa promedio vaya subiendo. 
La exploración nos permite juntar datos incluso
de las opciones más malas para tomar mejores decisiones en el futuro. 

La línea roja corresponde a la decisión de tomar la mejor decisión en cada
iteración. Nótese que este algoritmo alcanza una fracción de este nivel óptimo,
pues constantemente hace exploración (con probabilidad $\epsilon$) de otras
opciones.

Podemos ver cómo van compitiendo los brazos en el tiempo:

```{r, message = FALSE, warning=FALSE}
ggplot(df_iteraciones, aes(x=n, y= sumas / conteos, 
       group = brazo, colour=factor(brazo))) + geom_line()
```

Y podemos considerar intervalos para la estimación de la recompensa 
promedio de cada brazo (**Nota**: normalmente tendríamos que calcular
la desviación estándar de los datos de recompensas observados para
estimar el error estándar. Como en este caso estamos tratando con distribuciones
Poisson, podemos estimar la varianza como $\hat{lambda} / n$):

```{r}
df_iteraciones <- df_iteraciones %>% group_by(brazo) %>%
    mutate(promedio = sumas / conteos) %>%
  mutate(error_est = sqrt(promedio)/sqrt(conteos)) 
ggplot(df_iteraciones, aes(x=n, y=promedio, 
        ymin = promedio - 2*error_est,
        ymax = promedio + 2*error_est, 
                           colour=factor(brazo), group=brazo)) +
  geom_line() + geom_ribbon(alpha = 0.1)
```


### Comparación con experimentación clásica

En experimentación usual, siempre asignamos aleatoriamente el brazo (por una cantidad
de tiempo inicial fija), de forma que $epsilon=1$,
y después siempre escogemos el mejor brazo $epsilon = 0$. Es decir: al principio exploramos
intentando encontrar muestras balanceadas en tamaño para cada brazo, y al final
solo explotamos.


```{r}
set.seed(9322)
e_miope <- crear_epsilon_miope(epsilon = 0.1, inicial = 1, sim_fun = simular)
e_ab <- crear_epsilon_miope(epsilon = 1.0, inicial = 1, sim_fun = simular)
tiempo <- 500

df_iteraciones <- lapply(1:500, function(i){
  if(i==301){
    # si llegamos al tiempo 300 dejamos de explorar, y solo explotamos
    assign("epsilon", 0.0, environment(e_ab))
  }
  dat_miope <- e_miope()
  dat_ab <-  e_ab()
  dat_miope$tipo <- "miope"
  dat_ab$tipo <- "ab"
  bind_rows(dat_miope, dat_ab)
}) %>% bind_rows %>% as_tibble()
```

Veamos

```{r}
df_resumen <- df_iteraciones %>% group_by(n, tipo) %>% 
  summarise(promedio = sum(sumas) / sum(conteos))
ggplot(df_resumen, aes(x=n, y = promedio, group = tipo, colour = tipo)) +
  geom_line() + labs(title = "Desempeño")
```

**Observaciones**:

1. Nótese que en el periodo de experimentación, el algoritmo a/b tiene pérdidas comparado
con el algoritmo epsilon-miope. Esto es porque en toda esta etapa no tomamos las decisiones
usando las recompensas observadas.
2. Muy al principio, el algoritmo epsilon-miope pierde más que el a-b. Esto es porque al principio
está explotando un brazo muy malo. Una vez que explora los otros brazos, sin embargo, la explotación
mejora y supera en costo al método a/b.
3. Nótese que a largo plazo el algoritmo a/b superará en promedio al miope en recompensa promedio (una vez que dejamos de experimentar y ya identificamos correctamente la mejor opción). El algoritmo miope, sin embargo, si que explorando opciones malas una fracción del tiempo, lo cual quizá ya no sería necesario.

Podemos graficar también el acumulado, donde vemos de otra manera
la brecha entre los dos métodos en cuanto a recompensa acumulada en el tiempo:

```{r}
df_resumen <- df_iteraciones %>% group_by(n, tipo) %>% 
  summarise(acumulado = sum(sumas))
ggplot(df_resumen, aes(x=n, y = acumulado, group = tipo, colour = tipo)) +
  geom_line() + labs(title = "Desempeño")

```

**Preguntas**: 

- Estas gráficas muestran una corrida particular de recompensas observadas. Corre con
otra semilla para ver otras maneras en las que se pueden comportar estos algoritmos.
- ¿Cuál es el riesgo de correr por muy poco tiempo la fase de experimentación en el algoritmo
a/b?
- ¿Cuál es el riesgo de correr demasiado tiempo la fase de expermientación en el algoritmo a/b?
- ¿Cómo se vería en estas gráficas un algoritmo perfecto (ninguno tiene
mejor recompensa acumulada en ningún tiempo)? Añade estos límites teóricos (que en la práctica
no conociemos) a las gráficas de arriba.
- Compara el desempeño del algoritmo miope con distintos valores de $\epsilon$.

## Algoritmo softmax

```{r}
crear_softmax <- function(temperatura = 1, inicial = 1, sim_fun, recocido = F){
  n_brazos <- environment(sim_fun)$n_brazos
  conteos <- rep(0, n_brazos)
  recompensas <- vector("list", n_brazos)
  sumas <- rep(0, n_brazos)
  temperatura <- temperatura
  fun <- function(){
    if(recocido){
      temperatura <<- temperatura / log(t + 0.0001)
    }
    m <- max(sumas / temperatura)
    exp_sumas <- exp(sumas / temperatura - m)
    probs <- exp_sumas / sum(exp_sumas)
    brazo <- sample(1:n_brazos, 1, prob = probs)
    sim <- sim_fun(brazo)
    recompensas[[brazo]] <<- c(recompensas[[brazo]], sim)
    conteos[brazo] <<- conteos[brazo] + 1
    sumas[brazo] <<- sumas[brazo] + sim
    mejor <<- which.max(sumas /conteos)
  }
  fun
}
```

## El algoritmo de cota superior de confianza (UCB)

Los algoritmos mostrados arriba tienen el defecto de que no toman en cuenta
la incertidumbre que tenemos acerca de nuestra estimación de la recompensa
promedio de cada brazo.

